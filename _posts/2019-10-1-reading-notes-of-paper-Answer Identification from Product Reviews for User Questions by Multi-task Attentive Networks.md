---
title: Reading notes of paper "Improving the Explainability of Neural Sentiment Classifiers via Data Augmentation"
date: 2019-11-08
categories:
- Paper Digest
tags:
- Explainable Machine Learning
- Natural Language Processing
- Sentiment Analysis
---

### Paper

This paper "Improving the Explainability of Neural Sentiment Classifiers via Data Augmentation" is published in NIPS 2019 by Hanjie Chen, Yangfeng Ji.

### Background

Recently, many researchers have been working on sentiment classification, which is a hot sub-field in Natural Language Processing (NLP). Most of the approaches focus on achieve more remarkable performance on prediction accuracy, while the lack of classification interpretation may raise some issues in practice, such as interpretable and trustworthy. Ribeiro et al., 2016 demonstrated two desires characteristics of good explainers. First, the explanations must be interpretable. They not only need to provide some qualitative understanding between the inputs and predict responses but also are easy to be understood by human beings. Second, as it is difficult to achieve global fidelity for most of the black-box models, the explanations need to be local fidelity at least.

### Related Work

Currently, some researches work on the local fidelity explanation based on several individual model predictions. Ribeiro et al., 2016 proposed LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. The authors in this paper also use LIME to generate explanations. By assigning each feature an important value for a particular prediction, Lundberg et al., 2017 presented a unified framework for interpreting predictions, SHAP (SHarply Additive exPlanations). Furthermore, there are several model-based local explanation methods. Li et al., 2016 introduced methods for visualizing a unit's salience, the amount that it contributes to the final composed meaning from first-order derivatives. These methods can help us understand the compositionality and other semantic properties of deep networks. Bahdanau et al., 2015 designed an attention mechanism in the task of language translation. Following this work, many researchers also use the attention weights generated by the attention mechanism in some neural attention models to help explain the models.
In Gilplin et al., 2018, the authors discussed the importance of human evaluation, they claimed the purpose of the explainability of an explanation is to make humans easy understand or interpret. In this paper, the authors also design a human evaluation to evaluate the explainability of their generated explanations.

Many prior works use data augmentation as to whether to improve the prediction performances or to enhance model robustness. While the authors of this paper use data augmentation to improve the model explainability.

### Motivation

As the task of sentiment classification is to teach the model to make predictions by grasping sentiment words, the authors design two different data augmentation methods that create additional training examples to help the model focus on grasping sentiment words, in addition, they claim these data augmentation can also improve the model explainability.

### Algorithm

The first data augmentation method is using external knowledge (DA-EK). In the SentiWordNet (Baccianella et al., 2010), there are both sentiment words corpus and sentiment polarity scores for each word. And the sum of the three sentiment polarity scores for each word should be 1. For example, the word "truly" has a positive score 0.625, a negative score 0, and a neutral score 0.375. To generate additional augmenting examples, the authors remove the words which belong to the predefined sentiment word list in SentiWordNet from the original text. For example, the original text is "I like the movie." After removing the sentiment word, the new example is "I the movie."
The second data augmentation method is generating some adversarial examples (DA-ADV). Alzantot et al., 2018 developed an attack algorithm via genetic algorithms to generate adversarial examples. The authors use this method to generate adversarial examples in this paper. Furthermore, after generating the adversarial examples, they also remove sentiment words from these examples.
The following picture is two examples of data augmentation.

![](../../../../../assets/images/papernotesimgs/36.png){:style="margin:0 auto"}

With the additional training example, the authors also design a new training procedure. All the augmenting examples have a new label named "AUG". The original sentiment classifiers are binary classifiers, with this new label, the classifiers become three classes classifiers. Using the cross-entropy loss, a neural sentiment classifier is learned with the augmented training set; by optimizing the loss function, the classifier can achieve the best prediction accuracy on the augmented development set. While during testing, as there is no data augmentation in the test set, the trained neural classifier simply ignores any prediction on the label "AUG". In this training process, the sentiment classifiers focus on these sentiment words in the original text samples, in this situation, they can improve their explainabilities. In this paper, they do not introduce more details of this training process, and it is not very clear for me to understand.

### Experiments

The authors test their data augmentation methods with two neural sentiment classifiers, CNN (Kim et al., 2014) and LSTM (Hochreiter and Schmidhuber et al., 1997) on three benchmark datasets, SST (Socher et al., 2013), MR (Pang and Lee, 2005), and IMDB (Maas et al., 2011).
After training the two classifiers, they use two different methods to generate local explanations. The first method is LIME. The LIME selects top t words according to the weights of the linear approximation function based on the corresponding prediction. The second method is using cosine similarity in text representations. They use cosine similar function to calculate the similarity between a text and each word within this text and select the top t words which have higher similarity scores. Both of these two methods use these top t words as generated explanations.
The following figure is an example of generated explanations from two different models.

![](../../../../../assets/images/papernotesimgs/37.png){:style="margin:0 auto"}

With the results of the generated explanations, the authors also design two different evaluation methods. The first one is an automatic evaluation. The authors use the sentiment polarity scores from SentiWordNet to calculate the sentiment scores of the explanation word list. For example, the positive score of the explanation word list form model A is 0, while the score from model B is 0.635. With these sentiment scores, the authors define two conditions, if the explanation of a test example satisfies one of the two conditions, it gets one coherence score. In this situation, for collection explanations, the coherence score is the ratio of the number of coherent cases to the total number of instances. The following figure is the result of different classifiers on three benchmark datasets with different generative explanations methods.

![](../../../../../assets/images/papernotesimgs/38.png){:style="margin:0 auto"}

The second evaluation method is a human evaluation. Besides the human evaluation results can provide further justification of the coherence scores from the automatic evaluation. The human evaluation score is also calculated as the ratio of the sum of the scores to the number of examples. The following figure is the result of human evaluation. 

![](../../../../../assets/images/papernotesimgs/39.png){:style="margin:0 auto"}

From this result, we can see the score of human evaluation is larger than automatic evaluation. Because the SentiWordNet is not comprehensive enough, while human evaluators can always tell which explanation is better than the other. 

### Opinions

#### Strength

At first, the authors find a novel solution to improve the explainability of neural sentiment classifiers. Many researchers are working on how to generate explanations of the neural network models, while the authors move a further step to find a way to improve the explainability of models. To my best knowledge, they are the first to use the data augmentation to improve the explainability of neural sentiment classifiers. What is more, their experiments are well designed and sufficient. They try their methods on three different benchmark datasets and two different baseline models. And they also use two different methods to generate local explanations and design two different evaluation methods. Finally, from the results of their experiments, we can see their method can actually improve the model explainability more or less.

#### Weakness

In my opinion, there are several flaws in this paper. First, their work is built on several assumptions. But they do not give any proof of these assumptions. The first assumption in their approach is that the pre-defined sentiment words from SentiWordNet are also suitable for these three datasets. The SentiWordNet is the result of automatically annotating all WORDNET synsets according to their degrees of positivity, negativity, and neutrality, while the sentiment polarity may change in different contexts of situation. The second assumption is also related to pre-defined sentiment word polarities scores from SentiWordNet. To make the work more credible, the sentiment word polarities scores should be calculated again in these three different benchmark datasets. The third assumption is from using the cosine similarity method to generate explanations. They claim the sentiment polarity indicated by the top t similar words should be consistent with its overall sentiment
polarity. But they do not provide a proof for this assumption. So we can definitely doubt the explanations generated from this cosine similarity method.
What is more, as they claim using data augmentation can significantly improve the explainability of both neural classifiers, however, form the results of their experiments, we can see the improvement is very small.

#### Future works

As I am also working on sentiment analysis and we also explain our model by using attention weights from attention mechanism. This model-based local explanation method is not very easily understood by human beings. So in order to evaluate the explainability of our model, we can also use LIME to automatically generate the explanations and design automatic evaluation or human evaluation methods. What is more, this work is based on several assumptions, in the future, if there are some underlying assumptions, we need to prove it or avoid these assumptions.

### Reference

Alzantot, M., Sharma, Y., Elgohary, A., Ho, B.-J., Srivastava, M., and Chang, K.-W. (2018). Generating natural language adversarial examples. arXiv preprint arXiv:1804.07998.

Baccianella, S., Esuli, A., and Sebastiani, F. (2010). Sentiwordnet 3.0: an enhanced lexical resource for sentiment analysis and opinion mining.

Bahdanau, D., Cho, K., and Bengio, Y. (2015). Neural machine translation by jointly learning to align and translate. In ICLR.

Chen, Ji (2019). Improving the Interpretability of Neural Sentiment Classifiers via Data Augmentation. arXiv preprint arXiv:1909.04225. NeurIPS 2019

Gilpin, L. H., Bau, D., Yuan, B. Z., Bajwa, A., Specter, M., and Kagal, L. (2018). Explaining explanations: An overview of interpretability of machine learning. In 2018 IEEE 5th International Conference on Data Science and Advanced Analytics (DSAA), pages 80–89. IEEE.

Hochreiter, S. and Schmidhuber, J. (1997). Long short-term memory. Neural computation, 9(8):1735–1780.

Kim, Y. (2014). Convolutional neural networks for sentence classification. arXiv preprint arXiv:1408.5882.

Li, J., Chen, X., Hovy, E., and Jurafsky, D. (2016). Visualizing and understanding neural models in nlp. In Proceedings of NAACL-HLT, pages 681–691.

Lundberg, S. M. and Lee, S.-I. (2017). A unified approach to interpreting model predictions. In Advances in Neural Information Processing Systems, pages 4765–4774.

Maas, A. L., Daly, R. E., Pham, P. T., Huang, D., Ng, A. Y., and Potts, C. (2011). Learning word vectors for sentiment analysis. In Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies-volume 1, pages 142–150. Association for Computational Linguistics.

Pang, B. and Lee, L. (2005). Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. In Proceedings of the 43rd annual meeting on association for computational linguistics, pages 115–124. Association for Computational Linguistics.

Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. "Why should i trust you?: Explaining the predictions of any classifier." In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, pp. 1135-1144. ACM, 2016.

Socher, R., Perelygin, A., Wu, J., Chuang, J., Manning, C. D., Ng, A., and Potts, C. (2013). Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 conference on empirical methods in natural language processing, pages 1631–1642.